{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training diagram\n",
    "\n",
    "````{div} full-width\n",
    "```{mermaid}\n",
    "sequenceDiagram\n",
    "    autonumber\n",
    "    participant Agent\n",
    "    participant RL Method\n",
    "        Note left of RL Method: SVR, Actor-Critic...\n",
    "    participant Environment\n",
    "\n",
    "    loop Episode\n",
    "        Agent-->>+RL Method: Start training (Data, Initial State)\n",
    "        loop Step\n",
    "            RL Method-->>+Environment: Select an action following its exploration strategy\n",
    "            Environment-->>-RL Method: Return next state, action, reward and done flag\n",
    "            RL Method->>RL Method: Store transition to memory\n",
    "        end\n",
    "        RL Method->>RL Method: Update model\n",
    "        RL Method-->>-Agent: Returns episode reward\n",
    "    end\n",
    "```\n",
    "````"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Training an Agent powered by SVR model on 600 datasets split between binary classification, linear and poisson regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docs.workflows.utils.generate_training_datasets import generate_training_datasets\n",
    "datasets = generate_training_datasets(600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from ostatslib.agents import Agent\n",
    "from ostatslib.reinforcement_learning_methods import SupportVectorRegression\n",
    "\n",
    "agent = Agent(rl_method=SupportVectorRegression())\n",
    "for index, (dataset_type, dataset) in enumerate(datasets):\n",
    "    agent.train(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking Agent analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting first dataset for each trained dataset type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datacooker.recipes import LogitRecipe, PoissonRecipe, Recipe\n",
    "\n",
    "logistic_regression_dataset = [dataset for dataset_type, dataset in datasets if dataset_type == LogitRecipe][0]\n",
    "linear_regression_dataset = [dataset for dataset_type, dataset in datasets if dataset_type == Recipe][0]\n",
    "poisson_regression_dataset = [dataset for (dataset_type, dataset) in datasets if dataset_type == PoissonRecipe][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Binary classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: is_response_quantitative, reward: 0.75, next state features: [1 0 0 0 1 0 0]\n",
      "Action: get_log_rows_count, reward: 0.75, next state features: [1.         0.         0.15323742 0.         1.         0.\n",
      " 0.        ]\n",
      "Action: is_response_positive_values_only_check, reward: 0.75, next state features: [1.         0.         0.15323742 0.         1.         0.\n",
      " 1.        ]\n",
      "Action: is_response_discrete_check, reward: 0.75, next state features: [1.         0.         0.15323742 0.         1.         1.\n",
      " 1.        ]\n",
      "Action: is_response_dichotomous_check, reward: 0.75, next state features: [1.         0.         0.15323742 1.         1.         1.\n",
      " 1.        ]\n",
      "Action: LogisticRegressionCV(cv=5), reward: 0.7231884057971014, next state features: [1.         0.62318841 0.15323742 1.         1.         1.\n",
      " 1.        ]\n"
     ]
    }
   ],
   "source": [
    "analysis = agent.analyze(logistic_regression_dataset)\n",
    "\n",
    "for step in analysis:\n",
    "    print(f'Action: {step.result}, reward: {step.reward}, next state features: {step.state.features_vector}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: is_response_quantitative, reward: 0.75, next state features: [1 0 0 0 1 0 0]\n",
      "Action: get_log_rows_count, reward: 0.75, next state features: [1.         0.         0.20431554 0.         1.         0.\n",
      " 0.        ]\n",
      "Action: is_response_positive_values_only_check, reward: 0.75, next state features: [ 1.          0.          0.20431554  0.          1.          0.\n",
      " -1.        ]\n",
      "Action: SVR(), reward: 0.6955879307065417, next state features: [ 1.          0.79558793  0.20431554  0.          1.          0.\n",
      " -1.        ]\n"
     ]
    }
   ],
   "source": [
    "analysis = agent.analyze(linear_regression_dataset)\n",
    "\n",
    "for step in analysis:\n",
    "    print(f'Action: {step.result}, reward: {step.reward}, next state features: {step.state.features_vector}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Poisson Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: is_response_quantitative, reward: 0.75, next state features: [1 0 0 0 1 0 0]\n",
      "Action: get_log_rows_count, reward: 0.75, next state features: [1.         0.         0.23885728 0.         1.         0.\n",
      " 0.        ]\n",
      "Action: is_response_positive_values_only_check, reward: 0.75, next state features: [1.         0.         0.23885728 0.         1.         0.\n",
      " 1.        ]\n",
      "Action: is_response_discrete_check, reward: 0.75, next state features: [1.         0.         0.23885728 0.         1.         1.\n",
      " 1.        ]\n",
      "Action: is_response_dichotomous_check, reward: 0.75, next state features: [ 1.          0.          0.23885728 -1.          1.          1.\n",
      "  1.        ]\n",
      "Action: <statsmodels.genmod.generalized_linear_model.GLMResultsWrapper object at 0x7fa6103a1240>, reward: 0.8979244159479015, next state features: [ 1.          0.79792442  0.23885728 -1.          1.          1.\n",
      "  1.        ]\n"
     ]
    }
   ],
   "source": [
    "analysis = agent.analyze(poisson_regression_dataset)\n",
    "\n",
    "for step in analysis:\n",
    "    print(f'Action: {step.result}, reward: {step.reward}, next state features: {step.state.features_vector}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bc31b72d76bb86a03669e931e818811fb2ab46ea94e9eb5815941aa084bb6329"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
